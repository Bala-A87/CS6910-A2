{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transforms = Compose([Resize((224, 224)), ToTensor()])\n",
    "dataset = ImageFolder('../data/train/', transform=transforms)\n",
    "train_dataloader = DataLoader(dataset, 64, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from typing import List, Tuple\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a simple convolutional neural network, with 5 convolutional layers, each followed by maxpooling and optionally batchnorm, and two fully-connected layers.\n",
    "\n",
    "    Args:\n",
    "        filters (List[Tuple[int, int]]): Details of all the convolutional layers, each given by a tuple (num_filters, kernel_size), where num_filters is the number of convolutional filters and kernel_size is the size of the filter.\n",
    "        width_dense (int): The number of units in the hidden fully-connected/dense layer.\n",
    "        input_size (Tuple[int, int], optional): The size of the input images (images are assumed RGB, i.e., 3 channels).\n",
    "            Defaults to (224, 224).\n",
    "        activation_conv (torch.nn.Module, optional): The activation/non-linearity to use for the convolutional layers.\n",
    "            Defaults to torch.nn.ReLU.\n",
    "        activation_conv (torch.nn.Module, optional): The activation/non-linearity to use for the hidden dense layer.\n",
    "            Defaults to torch.nn.ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        filters: List[Tuple[int, int]],\n",
    "        width_dense: int,\n",
    "        input_size: Tuple[int, int] = (224, 224),\n",
    "        activation_conv: nn.Module = nn.ReLU,\n",
    "        activation_dense: nn.Module = nn.ReLU,\n",
    "        batch_norm: bool = True,\n",
    "        dropout: float = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=filters[0][0], kernel_size=filters[0][1], padding='same'),\n",
    "            activation_conv(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=filters[0][0]) if batch_norm else nn.Identity(),\n",
    "            nn.Conv2d(in_channels=filters[0][0], out_channels=filters[1][0], kernel_size=filters[1][1], padding='same'),\n",
    "            activation_conv(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=filters[1][0]) if batch_norm else nn.Identity(),\n",
    "            nn.Conv2d(in_channels=filters[1][0], out_channels=filters[2][0], kernel_size=filters[2][1], padding='same'),\n",
    "            activation_conv(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=filters[2][0]) if batch_norm else nn.Identity(),\n",
    "            nn.Conv2d(in_channels=filters[2][0], out_channels=filters[3][0], kernel_size=filters[3][1], padding='same'),\n",
    "            activation_conv(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=filters[3][0]) if batch_norm else nn.Identity(),\n",
    "            nn.Conv2d(in_channels=filters[3][0], out_channels=filters[4][0], kernel_size=filters[4][1], padding='same'),\n",
    "            activation_conv(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=filters[4][0]) if batch_norm else nn.Identity(),\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Identity() if dropout is None else nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=int(filters[4][0]*(input_size[0] * input_size[1])/32**2), out_features=width_dense),\n",
    "            activation_dense(),\n",
    "            nn.Identity() if dropout is None else nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=width_dense, out_features=10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(filters=[(16, 3), (32, 3), (64, 3), (128, 3), (64, 3)], width_dense=256, dropout=0.2).to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalAccuracy(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        return (y_pred.detach().cpu().argmax(dim=1) == y_true.cpu()).sum() / len(y_true.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)\n",
    "metric = CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/157]\tLoss: 2.304157, Score: 0.0781\n",
      "[2/157]\tLoss: 2.297452, Score: 0.1250\n",
      "[3/157]\tLoss: 2.296964, Score: 0.1094\n",
      "[4/157]\tLoss: 2.296141, Score: 0.1250\n",
      "[5/157]\tLoss: 2.305525, Score: 0.1094\n",
      "[6/157]\tLoss: 2.298108, Score: 0.1406\n",
      "[7/157]\tLoss: 2.294183, Score: 0.1094\n",
      "[8/157]\tLoss: 2.294595, Score: 0.1562\n",
      "[9/157]\tLoss: 2.293852, Score: 0.1250\n",
      "[10/157]\tLoss: 2.298033, Score: 0.0781\n",
      "[11/157]\tLoss: 2.289092, Score: 0.1406\n",
      "[12/157]\tLoss: 2.289972, Score: 0.2031\n",
      "[13/157]\tLoss: 2.282388, Score: 0.2031\n",
      "[14/157]\tLoss: 2.289270, Score: 0.1094\n",
      "[15/157]\tLoss: 2.264889, Score: 0.2344\n",
      "[16/157]\tLoss: 2.283979, Score: 0.1719\n",
      "[17/157]\tLoss: 2.269090, Score: 0.1562\n",
      "[18/157]\tLoss: 2.259615, Score: 0.2344\n",
      "[19/157]\tLoss: 2.258291, Score: 0.1875\n",
      "[20/157]\tLoss: 2.228948, Score: 0.2969\n",
      "[21/157]\tLoss: 2.306071, Score: 0.1406\n",
      "[22/157]\tLoss: 2.229174, Score: 0.2500\n",
      "[23/157]\tLoss: 2.231511, Score: 0.2344\n",
      "[24/157]\tLoss: 2.225268, Score: 0.1875\n",
      "[25/157]\tLoss: 2.257912, Score: 0.2031\n",
      "[26/157]\tLoss: 2.255679, Score: 0.1719\n",
      "[27/157]\tLoss: 2.260098, Score: 0.1562\n",
      "[28/157]\tLoss: 2.266216, Score: 0.1875\n",
      "[29/157]\tLoss: 2.249811, Score: 0.2188\n",
      "[30/157]\tLoss: 2.174274, Score: 0.3594\n",
      "[31/157]\tLoss: 2.193734, Score: 0.3125\n",
      "[32/157]\tLoss: 2.266902, Score: 0.2344\n",
      "[33/157]\tLoss: 2.282479, Score: 0.1562\n",
      "[34/157]\tLoss: 2.237876, Score: 0.1875\n",
      "[35/157]\tLoss: 2.203205, Score: 0.2812\n",
      "[36/157]\tLoss: 2.259170, Score: 0.1875\n",
      "[37/157]\tLoss: 2.273506, Score: 0.1562\n",
      "[38/157]\tLoss: 2.278870, Score: 0.1719\n",
      "[39/157]\tLoss: 2.256223, Score: 0.1875\n",
      "[40/157]\tLoss: 2.266402, Score: 0.1875\n",
      "[41/157]\tLoss: 2.228933, Score: 0.2031\n",
      "[42/157]\tLoss: 2.263733, Score: 0.1875\n",
      "[43/157]\tLoss: 2.192278, Score: 0.2969\n",
      "[44/157]\tLoss: 2.241266, Score: 0.2812\n",
      "[45/157]\tLoss: 2.253695, Score: 0.2344\n",
      "[46/157]\tLoss: 2.223694, Score: 0.2812\n",
      "[47/157]\tLoss: 2.192717, Score: 0.2969\n",
      "[48/157]\tLoss: 2.209217, Score: 0.2500\n",
      "[49/157]\tLoss: 2.228179, Score: 0.2031\n",
      "[50/157]\tLoss: 2.200674, Score: 0.2344\n",
      "[51/157]\tLoss: 2.219887, Score: 0.2031\n",
      "[52/157]\tLoss: 2.216749, Score: 0.2500\n",
      "[53/157]\tLoss: 2.226803, Score: 0.2500\n",
      "[54/157]\tLoss: 2.158285, Score: 0.3281\n",
      "[55/157]\tLoss: 2.201890, Score: 0.2500\n",
      "[56/157]\tLoss: 2.177374, Score: 0.2656\n",
      "[57/157]\tLoss: 2.154143, Score: 0.3281\n",
      "[58/157]\tLoss: 2.266373, Score: 0.1406\n",
      "[59/157]\tLoss: 2.231282, Score: 0.2500\n",
      "[60/157]\tLoss: 2.200510, Score: 0.2344\n",
      "[61/157]\tLoss: 2.256631, Score: 0.1562\n",
      "[62/157]\tLoss: 2.150714, Score: 0.2969\n",
      "[63/157]\tLoss: 2.221082, Score: 0.2500\n",
      "[64/157]\tLoss: 2.161331, Score: 0.2812\n",
      "[65/157]\tLoss: 2.198663, Score: 0.2500\n",
      "[66/157]\tLoss: 2.183757, Score: 0.2656\n",
      "[67/157]\tLoss: 2.196107, Score: 0.3125\n",
      "[68/157]\tLoss: 2.137194, Score: 0.3125\n",
      "[69/157]\tLoss: 2.203038, Score: 0.2812\n",
      "[70/157]\tLoss: 2.181723, Score: 0.2812\n",
      "[71/157]\tLoss: 2.144572, Score: 0.3281\n",
      "[72/157]\tLoss: 2.127086, Score: 0.3594\n",
      "[73/157]\tLoss: 2.189718, Score: 0.2500\n",
      "[74/157]\tLoss: 2.185781, Score: 0.2656\n",
      "[75/157]\tLoss: 2.165366, Score: 0.3281\n",
      "[76/157]\tLoss: 2.181995, Score: 0.3125\n",
      "[77/157]\tLoss: 2.155131, Score: 0.3281\n",
      "[78/157]\tLoss: 2.167096, Score: 0.3125\n",
      "[79/157]\tLoss: 2.216882, Score: 0.2500\n",
      "[80/157]\tLoss: 2.096885, Score: 0.4219\n",
      "[81/157]\tLoss: 2.231818, Score: 0.2031\n",
      "[82/157]\tLoss: 2.161894, Score: 0.2969\n",
      "[83/157]\tLoss: 2.180588, Score: 0.2969\n",
      "[84/157]\tLoss: 2.238622, Score: 0.2188\n",
      "[85/157]\tLoss: 2.162199, Score: 0.2969\n",
      "[86/157]\tLoss: 2.185246, Score: 0.2344\n",
      "[87/157]\tLoss: 2.169352, Score: 0.2812\n",
      "[88/157]\tLoss: 2.182862, Score: 0.2812\n",
      "[89/157]\tLoss: 2.156610, Score: 0.2969\n",
      "[90/157]\tLoss: 2.202117, Score: 0.2500\n",
      "[91/157]\tLoss: 2.158623, Score: 0.2500\n",
      "[92/157]\tLoss: 2.156027, Score: 0.2969\n",
      "[93/157]\tLoss: 2.208808, Score: 0.2344\n",
      "[94/157]\tLoss: 2.168739, Score: 0.3438\n",
      "[95/157]\tLoss: 2.227906, Score: 0.2344\n",
      "[96/157]\tLoss: 2.154673, Score: 0.3125\n",
      "[97/157]\tLoss: 2.228650, Score: 0.2344\n",
      "[98/157]\tLoss: 2.159512, Score: 0.3125\n",
      "[99/157]\tLoss: 2.205405, Score: 0.2344\n",
      "[100/157]\tLoss: 2.231210, Score: 0.2188\n",
      "[101/157]\tLoss: 2.221104, Score: 0.2344\n",
      "[102/157]\tLoss: 2.205410, Score: 0.2500\n",
      "[103/157]\tLoss: 2.184562, Score: 0.2656\n",
      "[104/157]\tLoss: 2.148715, Score: 0.3125\n",
      "[105/157]\tLoss: 2.248215, Score: 0.1875\n",
      "[106/157]\tLoss: 2.173329, Score: 0.2969\n",
      "[107/157]\tLoss: 2.155019, Score: 0.2812\n",
      "[108/157]\tLoss: 2.225434, Score: 0.2500\n",
      "[109/157]\tLoss: 2.205990, Score: 0.2500\n",
      "[110/157]\tLoss: 2.230601, Score: 0.2031\n",
      "[111/157]\tLoss: 2.231196, Score: 0.2188\n",
      "[112/157]\tLoss: 2.211070, Score: 0.2500\n",
      "[113/157]\tLoss: 2.136988, Score: 0.3125\n",
      "[114/157]\tLoss: 2.263346, Score: 0.1719\n",
      "[115/157]\tLoss: 2.177166, Score: 0.2656\n",
      "[116/157]\tLoss: 2.213511, Score: 0.2344\n",
      "[117/157]\tLoss: 2.209911, Score: 0.2344\n",
      "[118/157]\tLoss: 2.178018, Score: 0.2969\n",
      "[119/157]\tLoss: 2.249781, Score: 0.1562\n",
      "[120/157]\tLoss: 2.216854, Score: 0.2344\n",
      "[121/157]\tLoss: 2.153934, Score: 0.3125\n",
      "[122/157]\tLoss: 2.188671, Score: 0.2812\n",
      "[123/157]\tLoss: 2.245484, Score: 0.2031\n",
      "[124/157]\tLoss: 2.178447, Score: 0.3281\n",
      "[125/157]\tLoss: 2.240852, Score: 0.2188\n",
      "[126/157]\tLoss: 2.158490, Score: 0.3125\n",
      "[127/157]\tLoss: 2.186603, Score: 0.2656\n",
      "[128/157]\tLoss: 2.137836, Score: 0.3594\n",
      "[129/157]\tLoss: 2.186362, Score: 0.2969\n",
      "[130/157]\tLoss: 2.144284, Score: 0.2656\n",
      "[131/157]\tLoss: 2.238323, Score: 0.1719\n",
      "[132/157]\tLoss: 2.176445, Score: 0.3281\n",
      "[133/157]\tLoss: 2.098346, Score: 0.3750\n",
      "[134/157]\tLoss: 2.178011, Score: 0.2969\n",
      "[135/157]\tLoss: 2.179489, Score: 0.2656\n",
      "[136/157]\tLoss: 2.094411, Score: 0.3750\n",
      "[137/157]\tLoss: 2.148045, Score: 0.3125\n",
      "[138/157]\tLoss: 2.097655, Score: 0.3594\n",
      "[139/157]\tLoss: 2.136477, Score: 0.3594\n",
      "[140/157]\tLoss: 2.172710, Score: 0.2812\n",
      "[141/157]\tLoss: 2.132496, Score: 0.3438\n",
      "[142/157]\tLoss: 2.129441, Score: 0.3438\n",
      "[143/157]\tLoss: 2.194694, Score: 0.2812\n",
      "[144/157]\tLoss: 2.174554, Score: 0.2656\n",
      "[145/157]\tLoss: 2.215208, Score: 0.2344\n",
      "[146/157]\tLoss: 2.191453, Score: 0.2812\n",
      "[147/157]\tLoss: 2.150324, Score: 0.3281\n",
      "[148/157]\tLoss: 2.215276, Score: 0.2188\n",
      "[149/157]\tLoss: 2.129089, Score: 0.4062\n",
      "[150/157]\tLoss: 2.149873, Score: 0.2812\n",
      "[151/157]\tLoss: 2.183401, Score: 0.2500\n",
      "[152/157]\tLoss: 2.120728, Score: 0.3750\n",
      "[153/157]\tLoss: 2.112302, Score: 0.3750\n",
      "[154/157]\tLoss: 2.132693, Score: 0.3750\n",
      "[155/157]\tLoss: 2.141707, Score: 0.3281\n",
      "[156/157]\tLoss: 2.271763, Score: 0.1562\n",
      "[157/157]\tLoss: 2.217082, Score: 0.2000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    # loss = torch.zeros(EPOCHS)\n",
    "    # score = torch.zeros(EPOCHS)\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        y_pred = model(X)\n",
    "        loss_curr = loss_fn(y_pred, y)\n",
    "        score_curr = metric(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss_curr.backward()\n",
    "        optimizer.step()\n",
    "        print(f'[{batch+1}/{len(train_dataloader)}]\\tLoss: {loss_curr.cpu().detach():.6f}, Score: {score_curr.cpu():.4f}')\n",
    "        # loss[epoch] += loss_curr.cpu().detach()\n",
    "        # score[epoch] += score_curr.cpu()\n",
    "    # loss[epoch] /= len(train_dataloader)\n",
    "    # score[epoch] /= len(train_dataloader)\n",
    "    # print(f'[{epoch+1}/{EPOCHS}]\\tLoss: {loss[epoch]:.6f}, Score: {score[epoch]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
